---
title: "Hours Worked"
author:
  - affiliation: University of Wisconsin - Madison
    name: Simon Goring
date: "September 11, 2019"
output:
  html_document:
    code_folding: show
    fig_caption: yes
    keep_md: yes
    self_contained: yes
    theme: readable
    toc: yes
    toc_float: yes
dev: svg
highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 4, digits = 1)
```

## Actual Hours Worked

The data file is quite large, as with others.  This file was obtained from the Canada Open Data portal and represents [Actual hours worked by occupation, monthly, unadjusted for seasonality](https://open.canada.ca/data/en/dataset/f958af27-edcb-4337-a1ce-b7071486cc9d).

Here we will be looking at:

1.  How to connect safely and in a reproducible manner.
2.  How to manage data access in a way to save you time
3.  How normalization can save space & time

## Safely Connecting

Including your password in an open manner is often dangerous.  Especially if you use the same password for everything (looks at some of you. . . ).  When you share data across email it can also be frustrating or difficult to change passwords where needed, causing unintended problems.

One solution is to use a seperate file for your connection information.  This file will be included in your project directory, but not sent to your colleagues (and can be added to a `.gitignore` file if you are using `git` for version control).

The file needs to contain the information you will use to connect to your database (or other sensitive connections).

You can format the file any way you like, you just need to know how to read it into your script, and you need to let your colleagues know what format their file should be.

Lets use a plain text file for this project.  We'll call the file `db_connect.txt`.  The lines in the file represent (1) server, (2) port, (3) user, (4) password, (5) database name.

```
localhost
5432
postgres
postgres
nocdb
```

To then use this, I'll load the library, read in the file (saved in the same directory as the `Rmd` file) and then create the connection.  I'm going to use the `assertthat` package to just make sure that the database connection is there.

```{r connectdb}
library(RPostgreSQL)
library(dplyr)
library(readr)

connect <- readr::read_lines("db_connect.txt")

con <- RPostgreSQL::dbConnect(PostgreSQL(),
                              host = connect[1],
                              port = connect[2],
                              user = connect[3],
                              password = connect[4],
                              dbname = connect[5])

assertthat::assert_that(dbGetInfo(con)$host == connect[1],
           msg = "We're connecting somewhere weird. . . ")
```

The `assertthat` package has a set of functions that basically make assertions.  It's really good practice to use something like it to actually test your code as you write it.  It also lets you customize the error messages (using the `msg` parameter), so you can have meaningful errors, rather than "The function was provided incorrect information" or something less useful.

### Better, Faster Data Management

So, we want fully reproducible workflows, we want everything tied together, but we don't want to have to go through the same steps every time.

For this, planning is key.  What will your data/database look like once it's done?  Think about it a little bit.  In this case I expect a few things:

  1.  The database itself exists (tested above)
  2.  The database has all the data we need.
  3.  The data file exists.

If this is all, then we should be able to get started fairly easily.

#### Does the database have what we want?

I have looked at the data and expect several tables.  We can check that they're there.  This assumes that I've planned things out beforehand (hot take: you should plan things out beforehand)

```{r, checkValidDb}

expected_tables <- c("geography", "hourswork", "nationalocc", "gender", "adjtable")

validdb <- expected_tables %in% dbListTables(con)

if (!all(validdb)) {
  message(paste0("Some tables are missing! We're missing: ", expected_tables[!validdb]))
}
```

#### Do we have the data we need?

If some of the tables are missing (i.e., we get the message above), then we need to check to see that we have the original data we need to actually do our analysis.  So, is the file there?

We have the url for the file download, and we know the name of the actual file, so lets first check whether or not the file is there.  If it's not, we're going to download it from the URL:

```{r checkfile}

if (!all(validdb)) {

  #  We only need to do this if the database doesn't exist in its proper form!

  url <- "https://www150.statcan.gc.ca/n1/tbl/csv/14100300-eng.zip"
  filepath <- "data/"
  filename <- "14100300-eng/14100300.csv"
  zipname <- "14100300-eng.zip"

  csvfile <- file.exists(paste0(filepath, filename))
  zipfile <- file.exists(paste0(filepath, zipname))

  if (!csvfile) {
    message("The file isn't there! Checking for Zip file.")
    if (!zipfile) {
      message("Zip file isn't there, so we're going to download it.")
      download.file(url, paste0(filepath, zipname))
    }

    unzip(paste0(filepath, zipname),
          exdir = "data/14100300-eng")
  }

  assertthat::assert_that(file.exists(paste0(filepath, zipname)),
                          msg = "The Zip file didn't go where we expected.")
  assertthat::assert_that(file.exists(paste0(filepath, filename)),
                          msg = "The CSV files didn't go where we expected.")
}

```

So, now the zip file has been downloaded (but only if needed), and it's been extracted.  The file now exists.  We can read it in now.  To read things, we're going to use `readr`.  You may or may not need to install it already.

#### Loading in the Data

I'm loading in two versions of the data.  In general, if we weren't doing all the data description, I'd use an actual connection to the database, but this is partly for illustration:

```{r addTables}
data <- readr::read_csv('data/14100300-eng/14100300.csv')
data_short <- readr::read_csv('data/14100300-eng/14100300.csv', n_max = 10)
```

## Data Description

```{r, datasummary, echo = FALSE}

if (file.exists("data/output/summarydata.csv")) {

  dataoutput <- readr::read_csv("data/output/summarydata.csv")

} else {
  columns <- colnames(data_short)
  types <- apply(data_short, 2, class)

  # Takes a bit of time, because the file is so big!
  unique_vals <- apply(data, 2, n_distinct)

  dataoutput <- data.frame(columns = columns,
                           types = types,
                           distinct = unique_vals)
  readr::write_csv(dataoutput, "data/output/summarydata.csv")
}

DT::datatable(dataoutput, rownames = FALSE)

```

## Pushing Data to the Database

### GEO

The GEO field is a geographic table, containing `r n_distinct(data$GEO)` distinct values.  The `DGUID` field contains the same set of records, using an alphanumeric code.

```{r makeGeoTable}
if (!"geography" %in% dbListTables(con)) {
  geo_table <- data.frame(guid = 1:n_distinct(data$GEO),
                          geo  = unique(data$GEO))

  dbWriteTable(con,
               name = "geography",
               geo_table,
               row.names = FALSE)
} else {
  geo_table <- dbReadTable(con, name = "geography")
}

geo_objects <- data.frame(original = as.numeric(object.size(data$GEO)),
                               new = as.numeric(object.size(match(data$GEO,
                                                        geo_table$geo))))
```

We're just going to use a unique numeric ID here for geography.  By doing this we are reducing the geography field in the database from `r geo_objects$original` to `r geo_objects$new` bytes in size.

#### An Aside

As an aside here, I hate that I have to get the `object.size()` and then do so much with it.  It makes my code really ugly.  So, to clean it up I wrote a small function.  It takes in two parameters, the object I want to get the size of, and the units I want the size in.  I put it in the `R` directory as `R/sizer.R`.

```{r loadSizer}

source("R/sizer.R")

```

### Hours Worked

The field "Actual hours worked" contains `r n_distinct(data[,"Actual hours worked"])` levels.

```{r makeHoursTable}
if (!"hourswork" %in% dbListTables(con)) {
  hwork_table <- data.frame(hwid = 1:n_distinct(data[,"Actual hours worked"]),
                          worked = unique(data$`Actual hours worked`))

  dbWriteTable(con, name = "hourswork", hwork_table, row.names = FALSE)
} else {
  hwork_table <- dbReadTable(con, "hourswork")
}
```

### National Occupational Classification (NOC)

```{r makeNOCTable}
if (!"nationalocc" %in% dbListTables(con)) {
  noc_table <- data.frame(nocid = 1:n_distinct(data[,"National Occupational Classification (NOC)"]),
                          nationalocc = unique(data$`National Occupational Classification (NOC)`))

  dbWriteTable(con, name = "nationalocc", noc_table, row.names = FALSE)
} else {
  noc_table <- dbReadTable(con, "nationalocc")
}
```

### Gender

```{r makeGenTable}
if (!"gender" %in% dbListTables(con)) {
  gender_table <- data.frame(genid = 1:n_distinct(data[,"Sex"]),
                          gender = unique(data$Sex))

  dbWriteTable(con, name = "gender", gender_table, row.names = FALSE)
} else {
  gender_table <- dbReadTable(con, "gender")
}
```

### Values

```{r, matchingTables}
if (!"adjtable" %in% dbListTables(con)) {

  if (file.exists("data/output/long_table.csv")) {
    new_db <- readr::read_csv("data/output/long_table.csv")
  } else {
    geo_match <- match(data$GEO, geo_table$geo)
    hour_match <- match(data$`Actual hours worked`,
                  hwork_table$worked)
    occ_match <- match(data$`National Occupational Classification (NOC)`,
                       noc_table$nationalocc)
    gen_match <- match(data$Sex, gender_table$gender)
    values <- as.numeric(data$VALUE)

    new_db <- data.frame(geo = geo_match,
                         hours = hour_match,
                         occupation = occ_match,
                         gender = gen_match,
                         value = values)
    readr::write_csv(new_db, "data/output/long_table.csv")
  }
  dbWriteTable(con, name = "adjtable", new_db, row.names = FALSE)
} else {
  new_db <- dbReadTable(con, "adjtable")
}

cols <- c("GEO", "Actual hours worked", "National Occupational Classification (NOC)", "Sex", "VALUE")
dbsize <- data.frame(original = sizer(data[,cols]),
                     new = sizer(new_db))
```

So in the end, we've reduced the core of the database from `r dbsize$original` to `r dbsize$new` bytes.

## Putting it back together

Our data is now in a database that is smaller than the original size, and the data is managed more efficiently (by the DBMS) than R can it in memory.  Unfortunately, on the surface of things, it's a bit harder to understand:

```{r selectAdj}

query <- "SELECT * FROM adjtable LIMIT 10"
res <- dbGetQuery(con, query)
DT::datatable(res)
```

So to make sense of things again, we need to be clear about what we're looking for.  Lets say we want to find the occupation with the most weekly hours worked by women in Canada.  We need to know that:

**Sex** is *Females* in table **gender**
**geo** is *Canada* in table **geography**

And so we need to join the `gender` table together, and the `geography` table.  But they only share a common ID field within `adjtable`, which has both a `geo` and `gender` field.

So, we want to `SELECT` the `value` field from `adjtable`, along with the `hours` field:

```sql
SELECT avg(value) AS hours,
         COUNT(*) AS records,
      nationalocc AS occupation
FROM  adjtable AS adj
INNER JOIN   geography AS g   ON    g.guid = adj.geo
INNER JOIN      gender AS gen ON gen.genid = adj.gender
INNER JOIN   hourswork AS hw  ON   hw.hwid = adj.hours
INNER JOIN nationalocc AS noc ON noc.nocid = adj.occupation
WHERE          g.geo = 'Canada'
      AND gen.gender = 'Females'
	  AND value IS NOT NULL
    AND  hw.worked = 'Average actual hours (all workers, main job)'
GROUP BY nationalocc
ORDER BY hours DESC
```

To save space in this document (in the future), we're going to put the SQL script in its own file in the `sql` directory, saving it as `sql/top_hours.sql`.

```{r callSQL}
hoursql <- readr::read_file('sql/top_hours.sql')
callhrs <- dbGetQuery(con, hoursql)
```

This shows us the `r callhrs$occupation[1]` had the most hours worked by women in Canada, with, on average, `r round(callhrs$hours[1],1)` worked weekly.
